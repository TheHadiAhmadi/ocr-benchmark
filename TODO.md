# Persian OCR, Roadmap and first steps WBS

## First 3 steps

1. Create a Benchmark Dataset (DONE)
     - Gather a variety of Persian documents
     - Annotate ground-truth text, bounding boxes, and metadata. COCO
     - Store data in a standard format

2. Establish Success Criteria & Metrics
   - Accuracy Metrics: Character Error Rate (CER), Word Error Rate (WER), Segmentation error rate (SER), LLM correction metric. (DONE)
   - Performance Metrics: Inference speed, memory usage, CPU/GPU utilization. (DONE)
   - Qualitative Metrics: Ease of integration, ease of tuning for Persian language nuances.

3. Make a Data-Driven Decision
   - Based on the benchmark results, select the best performing tool(s) for integration.
   - Assess whether existing OCR models require fine-tuning or retraining.
   - Document the decision process and criteria

## Required Benchmarks

- Text Segmentation (Easy, Paddle, Yolo, CRAFT etc)
- Text OCR (Easy, Paddle, TrOCR, LLVM)
- Number OCR (Easy, Paddle, TrOCR, LLVM)

- LLM typo Correction for Persian language (ChatGPT, Deepseek, Qwen, GEMINI, Lens)

- LLVM image classification for persian documents

## List of Open-Source and Closed-Source to Benchmarking

- EASY OCR
- Paddle
- Tesseract OCR
- Keras-OCR
- Calamari OCR
- TrOCR
- YOLO

- ChatGPT
- ABBYY
- Microsoft Azure Computer Vision
- Amazon Textract
- Google Cloud Vision
- Google Lens

## Proposed Workflow (High-Level)

This workflow outlines a structured, iterative pipeline for Persian OCR.
As an initial baseline version, it has not yet been benchmarked and serves as
the foundational framework for future development and refinement.

1. **Image Preprocessing**
   - Deskew images, remove noise, correct rotation angles, and optimize contrast.
2. **Image Classification**
   - Categorize images (e.g., documents, scenes, mixed content) for targeted processing.
3. **Intent Detection**
   - Determine the image's purpose (e.g., invoice processing, text archiving) using LLM analysis of text and metadata.
4. **Image Segmentation**
   - Segment image regions into text blocks, logos/icons, headers/footers, and body content using AI models.
5. **Text OCR**
   - Extract text from segmented regions with Persian script support (right-to-left, ligatures).
6. **Logo and icon Detection**
   - Detect and catalog logos, symbols, and cultural elements specific to Persian content.
7. **Data Merging**
   - Merge OCRs outputs, logos, and metadata into a unified structured format (e.g., JSON/XML).
8. **LLM-Based Text Correction**
   - Apply LLMs to refine OCR text using Persian language context and grammar rules.
9. **Number Correction**
   - Validate and correct numerals (e.g., Eastern/Western Arabic digits, dates, codes) with rule-based checks, proximity analyse.
10. **Standardized Output**
    - Generate standardized output with corrected text, bounding boxes, confidence scores, and labels.
11. **Iterative Improvement**
    - Benchmark accuracy (CER/WER), test improvements, refine pipeline, and repeat validation.

# Too Long, Don't Read

Following is WBS based on above document generated By AI corrected by human

## **Phase 1: Benchmark Dataset & Evaluation Framework**
### **Objective**: Create structured benchmark data and metrics for Persian OCR evaluation.
#### **Tasks**:
1.1. **Data Collection**:
   - Gather Persian text images (printed/handwritten, scans, social media) with font/resolution/noise diversity.
1.2. **Annotation**:
   - Label ground-truth text, numbers, and bounding boxes.
   - Add metadata (font type, image quality, text direction).
1.3. **Dataset Structuring**:
   - Organize into train/validation/test splits (80/10/10).
   - Convert to standard formats (COCO/ICDAR) for tool compatibility.
1.4. **Metric Design**:
   - Implement CER/WER calculators.
   - Define speed/resource metrics (FPS, CPU/GPU usage).
   - Document Persian-specific edge cases (ligatures, diacritics).

#### **Deliverables**:
- Curated Persian OCR benchmark dataset.
- Automated evaluation scripts (CER/WER, speed profiling).
- Persian OCR challenge checklist (e.g., RTL text, overlapping characters).

---

## **Phase 2: Tool Benchmarking & Validation**
### **Objective**: Execute reproducible benchmarks across OCR tools.
#### **Tasks**:
2.1. **Tool Setup**:
   - Configure open-source tools (Tesseract, EasyOCR, TrOCR) with Persian language packs.
   - Integrate cloud APIs (Google Vision, Azure) with cost tracking.
2.2. **Benchmark Execution**:
   - Run text detection (YOLO, PaddleOCR) and OCR accuracy tests.
   - Test LLM correction (ChatGPT) for Persian text refinement.
   - Validate number extraction accuracy (Eastern/Western numerals).
2.3. **Result Logging**:
   - Record CER/WER, speed, and resource usage.
   - Document failures (e.g., misread ligatures, skewed text).

#### **Deliverables**:
- Benchmark configuration files (per tool).
- Results repository (spreadsheets/JSON with metrics).

---

## **Phase 3: Decision & Roadmap Definition**
### **Objective**: Analyze results and define implementation strategy.
#### **Tasks**:
3.1. **Comparative Analysis**:
   - Rank tools by accuracy (CER/WER), speed, and cost.
   - Highlight Persian-specific performance gaps (e.g., diacritic errors).
3.2. **Retraining Assessment**:
   - Evaluate feasibility of fine-tuning models (e.g., TrOCR) on Persian data.
   - Estimate compute/data requirements for custom training.
3.3. **Roadmap Finalization**:
   - Choose tools (e.g., "Tesseract + ChatGPT correction").
   - Define next steps: error-correction pipelines, model tuning, or API integration.

#### **Deliverables**:
- Benchmark analysis report (visualized rankings, trade-offs).
- Persian OCR implementation roadmap.
- Retraining feasibility memo (if applicable).

---

### **Rationale**:
- **Phase 1** consolidates all foundational work: data, annotation, and metric design.
- **Phase 2** focuses purely on execution and validation of benchmarks.
- **Phase 3** eliminates ambiguity by forcing decisive outputs (tool selection, clear next steps).

